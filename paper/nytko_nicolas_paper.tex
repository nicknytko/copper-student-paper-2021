\documentclass[review]{siamart190516}

% 1. Preamble and packages
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\ifpdf%
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi
\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{bm}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normtwo}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\mat}[1]{\bm{{#1}}}
\renewcommand{\vec}[1]{\bm{{#1}}}
\newcommand{\lequiv}{\Leftrightarrow}
\newcommand{\bigO}[1]{\mathcal{O}\!\left(#1\right)}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\sfrac}[2]{#1/#2}
\newcommand{\hquad}{\enskip}
\newcommand{\expected}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\mspan}[1]{\text{span}\left( #1 \right)}
\newcommand{\prob}[1]{P\left(#1\right)}
\newcommand{\probt}[1]{P\left( \text{#1} \right)}
\newcommand{\condprob}[2]{P\left(#1 \:|\: #2\right)}
\newcommand{\condprobt}[2]{P\left(\text{#1} \:|\: \text{#2}\right)}
\newcommand{\bayes}[2]{\frac{\condprob{#2}{#1}\prob{#1}}{\prob{#2}}}
\newcommand{\bayesx}[3]{\frac{\condprob{#2}{#1}\prob{#1}}{\condprob{#2}{#1}\prob{#1} + \condprob{#2}{#3}\prob{#3}}}
\newcommand{\sech}{\text{sech}}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\vect}[2]{\underline{{#1}}_{{#2}}}
\newcommand{\basisp}[1]{\underline{{p}}_{{#1}}}
\newcommand{\basisq}[1]{\underline{{q}}_{{#1}}}
\newcommand{\coeff}[1]{\underline{{a}}_{{#1}}}
\newcommand{\bestfit}{\underline{\bar{x}}}
\newcommand{\grad}{\nabla}
\newcommand{\laplace}{\Delta}

% 2. Paper title
\newcommand{\TheTitle}{%
  A Supervised Learning Approach to Predict Multigrid Convergence
}

% 2.5. Short title for running heads (if needed)
\newcommand{\TheShortTitle}{%
  \TheTitle
}

% 3. Student Name
\newcommand{\TheName}{Nicolas Nytko}

% 4. Student Address
\newcommand{\TheAddress}{%
  University of Illinois at Urbana-Champaign,
  (\email{nnytko2@illinois.edu}).
}

% 5. Acknowledge funding or other resources
\newcommand{\TheFunding}{%
  This work was funded by IJK\@.
}

% 6. Collaborators, such as advisor or research collaborators
\newcommand{\TheCollaborators}{%
  Matthew West,
  Luke Olson,
  Scott MacLachlan
}

% ---------------------------------------------
% ---------------------------------------------
\author{\TheName\thanks{\TheAddress}}
\title{{\TheTitle}}
\headers{\TheShortTitle}{\TheName}
\ifpdf%
\hypersetup{%
  pdftitle={\TheTitle},
  pdfauthor={\TheName}
}
\fi

\begin{document}

\maketitle

\begin{center}
In collaboration with:
  {\TheCollaborators}
\end{center}
\vspace{1cm}
% ---------------------------------------------
% ---------------------------------------------

\begin{abstract}
  \input{../abstract/abstract.txt}
\end{abstract}

\begin{keywords}
  convergence, relaxation weight, machine learning, supervised learning, graph net, cnn, convolutional network
\end{keywords}

\section{Introduction}\label{sec:intro}

Multigrid methods are some of the most widely used linear solvers for sparse systems -- particularly ones that arise from the discretization of partial differential equations.  Standard geometric multigrid can be very effective when optimal coarsening is chosen, but is much less so when anisotropy is present, and becomes extremely difficult to use effectively on unstructured meshes, for example those generated from finite element discretizations.  Algebraic Multigrid attempts to solve this by exploiting the structure of the system itself to create an interpolation operator.  This ``optimal'' operator is often generated using heuristics that must be tuned and tweaked for a specific problem.  An example of which is Ruge-St\"{u}ben coarsening, which generates a prolongation and interpolation operator using strength of connection, treating the original sparse system as a graph.  Intuitively, there must be some sense of a cutoff as to how strong the connection must be before a point is considered coarse or fine.  This is one example of a parameter that must be tuned when using multigrid, algebraic or not.  The effectiveness of the parameter set is hard to understand, and most often consists of actually running the multigrid iterations.

In this paper, a framework for predicting the resulting convergence rates is introduced.  This framework takes as input a C/F splitting of the mesh, the output of AMG, and outputs a rate of convergence between $0$ and $1$, inclusive.  A lower value indicates a ``faster'' rate of convergence -- more of the error is dissipated at each iteration.  Depending on the problem, either a convolutional neural network or a graph net is trained, using a supervised learning approach.  Random perturbations of C/F mesh splittings are created and evaluated, and then used to train some sort of neural network.  The problems discussed are: $(1)$ a 1D variable coefficient Poisson equation, and $(2)$ a 2D recirculating flow convection-diffusion problem with specific parameters.

\section{Methods}\label{sec:methods}

The methods described here will be split into two subsections describing each problem that was explored. First, the Poisson case and the data generation is discussed (Section \ref{subsec:methods_poisson}) followed by the respective convolutional neural network that was trained (Section \ref{subsec:poisson_cnn}).  Afterwards, data generation for the 2D convection-diffusion problem are introduced (Section \ref{subsec:methods_conv}) and an overview of the trained convolutional network is given (Section \ref{subsec:conv_cnn}).  The use of two graph nets is also described: one that performs a simple edge convolution for each node (Section \ref{subsec:conv_gcn}), and another that implements ``message passing'' and learns optimal edge weighting of the convolution (Section \ref{subsec:conv_mpnn}).

\subsection{1D Poisson}\label{subsec:methods_poisson}

Formally, the problem being solved is the Poisson equation in one dimension with variable coefficients:

\begin{equation} \label{eqn:poisson}
  -\grad \cdot \left(k\left(\vec{x}\right) \grad\vec{u}\right) = f
\end{equation}

With the right hand side $f\left(\vec{x}\right)=\vec{x}$ being arbitrarily chosen.  \ref{eqn:poisson} is discretized using finite differences on a grid of $N=31$ internal points on the domain $\Omega = \left[-1, 1\right]$ and Dirichlet boundary conditions, $\partial\Omega = 0$.  To preserve the symmetric, positive definite properties of the resulting linear system, the $k\left(\vec{x}\right)$ function is discretized on the grid \textit{midpoints}.

Before the neural network can be trained, a dataset of approximately 300,000 random C/F grid splittings and their computed convergence rate and optimal relaxation weight was generated.  A set of $6$ reference grids were first created according to various ``coarsening'' factors:

\begin{equation} \label{eqn:poisson_coarsening}
  r = \begin{Bmatrix} 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\end{Bmatrix}
\end{equation}

With each of the values in \ref{eqn:poisson_coarsening}, say $r_i = j$ refers to a grid in which each $j$th point is coarse, and the rest fine.  So, a coarsening by $3$ is a grid that has roughly $\frac{1}{3}$ of its distribution of points as coarse and all others fine.  Each of these reference grids was randomly permuted such that each grid point had a random probability of being flipped to the opposite value -- coarse point to fine and fine point to coarse.  For each reference grid, approximately 1000 random trials of every following probability were run:

\begin{equation} \label{eqn:poisson_probabilities}
  p = \begin{Bmatrix} 0.01 & 0.05 & 0.1 & 0.25 & 0.5 & 0.75 \end{Bmatrix}
\end{equation}

Additionally, each trial was run with a randomly-chosen function for the variable coefficients $k\left(\vec{x}\right)$.  These are chosen from the set below, with coefficient values specifically chosen to prevent the function from being non-positive at any value in $\Omega$.

\begin{equation} \label{eqn:poisson_varcoefficients}
k\left(x\right) = \begin{cases}
\alpha & 0 < \alpha < 10 \\
\text{rand()}\left(\alpha + 1\right) & 0 < \alpha < 10 \\
\alpha\cos\left(\pi x \beta\right) + \gamma & 0 < \alpha < 10, 0 < \beta < 10, 0 < \gamma < 10\\
\left(\sum_{i=1}^5 \alpha_i x^i\right) + 0.01 & -10 < \alpha < 10
\end{cases}
\end{equation}

The full set of C/F splittings is then given to a 2 level V-cycle Multigrid solver that is run for a maximum of 15 iterations.  The solver is composed of one round of weighted Jacobi pre-smoothing, a coarse error correction, then another round of Jacobi post-smoothing.  At each iteration, the absolute error between the approximation and the ``exact'' solution (pre-computed via sparse linear solve) is found and saved: $e_i = \norm{\mat{A}^{-1}\vec{f} - \vec{u}_i}$.  This sequence of errors is then used to compute the average convergence rate.  To acquire the optimal relaxation weight, the smoother is run through a bracketed numerical optimization method with the assumption that the convergence rate is unimodal as a function of relaxation weight. \footnote{Experimental testing generally asserts this to be true, however this will remain a conjecture for now.}

\subsection{Poisson CNN}\label{subsec:poisson_cnn}

A deep convolutional network with residual connections was trained to predict both the optimal relaxation weight and convergence rate.  A textual overview of the CNN is described here:

\begin{enumerate}
\item 6 1D CNN layers of kernel 7, input two channels output 7 channels.  Padding by three on each side to keep dimensions.
\item 6 CNN layers of kernel 5, input/output 7 channels.  Padding by two on each side.
\item 6 CNN layers of kernel 3, input/output 7 channels.  Padding by 1 on each side.
\item Max-pooling layer of kernel 2, stride 2.  Effectively reduces input size by half.
\item 8 Fully-connected layers to gradually reduce output to a 2-vector describing the convergence and relaxation weight.
\end{enumerate}

Each layer is followed by an implicit ReLU nonlinear activation function.  Because the convolutional layers (except for the first) keep the input and output size static, we are able to push residual values and skip layers similarly to a ResNet.  Even numbered layers $n$ take as input both the output of layers $n-1$ and $n-2$, while odd-numbered layers only use the output from layer $n-1$.

The C/F splitting is remapped such that a coarse point is given a value of $1$ and a fine point the value of $-1$.  The variable coefficients were also re-discretized to be defined on the nodal points instead of midpoints in order for the splitting and coefficients to be represented by vectors of same length.  These two were then stacked into a two-channel tensor for input into the CNN.  When training, input values are normalized to be within the range of $\left[0,1\right]$.  This normalization is undone when output is displayed.

\subsection{2D Convection-Diffusion}\label{subsec:methods_conv}

This specific problem models what is called the \textit{double glazing problem}, modeling the temperature distribution of a cavity with a single ``hot'' wall.  This is given by the equation:

\begin{equation}\label{eqn:conv}
  -k\grad^2u + \vec{w}\cdot\grad u = f
\end{equation}

The wind function, $\vec{w}\left(x,y\right)$ is defined as $\vec{w}\left(x,y\right)=\begin{bmatrix} 2y\left(1-x^2\right) & 2x\left(1-y^2\right) \end{bmatrix}$.  The domain is a square of side length two centered at the origin, $\Omega = \left[-1,1\right] \times \left[-1,1\right]$.  Dirichlet boundary conditions are defined on $\partial \Omega$, with the one ``hot'' wall defined on $x=1$ with value $\partial \Omega_H=1$.  The other boundaries are ``cold'' walls with $\partial \Omega_c=0$.  A diffusivity constant of $k=0.1$ is used.

The above PDE is discretized using finite-elements on a structured grid of 25x25 internal points.  Using a grid as a basis for the discretization allows use of both CNN and more sophisticated graph convolutional techniques.

Generating a dataset of mesh splittings and convergence rates is done in an overall similar way to the 1D Poisson equation with a few notable differences.  Again, a set of reference C/F splittings were generated that are later permuted.  For the convection-diffusion case, the following reference splittings were used:

\begin{enumerate}
\item All fine points
\item All coarse points
\item Splitting as given by Ruge-St\"{u}ben coarsening ($\theta=0.25$)
\item Coarsening in each direction by 2
\item Coarsening in each direction by 3
\item Coarsening in each direction by 4
\item Coarsening in each direction by 5
\end{enumerate}

The entries of each individual reference splitting were then randomly permuted according to a defined probability.  The probability values used are the same as those in the Poisson case, repeated here for convenience:

\begin{equation} \label{eqn:conv_probabilities}
  p = \begin{Bmatrix} 0.01 & 0.05 & 0.1 & 0.25 & 0.5 & 0.75 \end{Bmatrix}
\end{equation}

This generated set of C/F splittings is passed along to another 2 level V-cycle multigrid solver run for a maximum of 50 iterations.  This solver performs two rounds of Jacobi pre-and-post smoothing, with a coarse error correction between the smoothing steps.  The absolute error between the approximation at each iteration and the ``exact'' solution is computed and saved, with the full sequence used to compute the average convergence rate.  Note the optimal Jacobi relaxation weight is not found here, as experimentation found that the optimal weight would nearly always be $1$.

\subsection{Convection-Diffusion CNN}\label{subsec:conv_cnn}

A 2D convolutional network was trained to predict convergence when given a C/F splitting for the specific recirculating flow problem.  Since the input parameters are slightly less complex, a less deep (\textit{shallower}, if you will) network was trained:

\begin{enumerate}
\item 3 2D CNN layers of kernel 7, input two channels output 7 channels.  Padding by three on each side to keep dimensions.
\item 3 CNN layers of kernel 5, input/output 7 channels.  Padding by two on each side.
\item 3 CNN layers of kernel 3, input/output 7 channels.  Padding by 1 on each side.
\item 2D Max-pooling layer of kernel 2, stride 2.  Effectively reduces input size by half.
\item 1 Fully-connected layers to reduce output to a scalar predicting convergence rate.
\end{enumerate}

Each layer is followed by a ReLU nonlinear activation function.  Odd layers are fed the output of the previous two layers, while even layers are fed the input of only the previous layer, similarly to a ResNet.  C/F splittings are mapped so that coarse points have value $1$ and fine points have value $-1$.  The CNN thus has a $25\times 25=625$-length vector as input.  Interestingly, normalization of the convergence rates is unneeded as the minimum and maximum recorded values are already close to $0$ and $1$, respectively.

\subsection{Convection-Diffusion Graph Convolutional Network (GCN)}\label{subsec:conv_gcn}

The main downside of using traditional convolutional layers is that they are useful only on structured, grid-like inputs.  Grid-based convolution is ineffective on the more complex meshes that may arise from finite-element discretizations.  By treating the input mesh as a graph and using graph-based convolution techniques, we may get around this roadblock.  The first graphnet that was tried uses the GCN layer introduced by Kipf and Welling\cite{gcn} and is described here, the other uses message passing and is described in Section \ref{subsec:conv_mpnn}.

For the graphnets, the sparse FEM system was re-interpreted as a graph by assigning each row/column to be a node and defining connectivity between nodes as nonzero matrix entries.  Edge weights were taken to be the entry values themselves, although normalized to be within the range of $[0, 1]$.

The Graph Convolutional Network (GCN) layer is defined as an operator on the graph Laplacian, using the following propagation rule:
\begin{equation}
  \mat{H}^{\left(i+1\right)} = \sigma\left( \mat{\tilde{D}}^{-\frac{1}{2}} \mat{\tilde{A}} \mat{\tilde{D}}^{-\frac{1}{2}} \mat{H}^{\left(i\right)} \mat{W}^{\left(i\right)} + \vec{b}^{(i)} \right)
\end{equation}
With $\mat{H}^{\left(i\right)}$ being the $i$'th hidden layer, $\mat{\tilde{A}} = \mat{A} + \mat{I}$ the adjacency matrix with added self loops, $\tilde{d}_{ii}=\sum_j\tilde{a}_{ij}$ a diagonal matrix consisting of the sum of outgoing edge weights, $\mat{W}^{(i)}$ the weight matrix at layer $i$, and $\sigma\left(\cdot\right)$ some nonlinear activation function.  Hidden layers have dimensions $n \times f$, with $n$ the number of graph nodes and $f$ the number of features at each node.  Thus, using only GCN layers it is not possible to reduce the number of rows in hidden layers (or the input layer for that matter).

Assume $\sigma\left(\cdot\right) = \text{ReLU}\left(\cdot\right) = \text{max}\left\{\cdot, 0\right\}$ for all activation functions.  Let $\mat{X}$ be the $n \times 1$ vector containing C/F splitting values for each node.  The network that was trained for this application had the following architecture:
\begin{align*}
\mat{H}^{(1)} &= \sigma\left( \mat{D}^{-\frac{1}{2}} \mat{A} \mat{D}^{-\frac{1}{2}} \mat{X} \mat{W}^{(1)} + \vec{b}^{(1)} \right) \in \mathbb{R}^{n \times 2} \\
\mat{H}^{(2)} &= \sigma\left( \mat{D}^{-\frac{1}{2}} \mat{A} \mat{D}^{-\frac{1}{2}} \mat{H}^{(1)} \mat{W}^{(2)} + \vec{b}^{(2)}\right) \in \mathbb{R}^{n \times 3}\\
  \mat{H}^{(3)} &= \sigma\left( \mat{D}^{-\frac{1}{2}} \mat{A} \mat{D}^{-\frac{1}{2}} \mat{H}^{(2)} \mat{W}^{(3)} + \vec{b}^{(3)}\right) \in \mathbb{R}^{n \times 2}\\
  \mat{H}^{(4)} &= \sigma\left( \mat{D}^{-\frac{1}{2}} \mat{A} \mat{D}^{-\frac{1}{2}} \mat{H}^{(3)} \mat{W}^{(4)} + \vec{b}^{(4)}\right) \in \mathbb{R}^{n \times 1}\\
\mat{R} &=
\begin{bmatrix}
\hdots & \mat{X}^T & \hdots \\
\hdots & \left(\mat{H}^{(1)}\right)^T & \hdots \\
\hdots & \left(\mat{H}^{(2)}\right)^T & \hdots \\
\hdots & \left(\mat{H}^{(3)}\right)^T & \hdots \\
\hdots & \left(\mat{H}^{(4)}\right)^T & \hdots \\
\end{bmatrix} \\
y &= \sum_{j=1}^n \sigma\left( \sigma\left(\mat{r}_j\mat{W}^{(5)} + \vec{b}^{(5)}\right) \mat{W}^{(6)} + \vec{b}^{(6)}\right)
\end{align*}
Note that the matrix $\mat{R}$ is formed whose columns are the propagation history of each node; this is an attempt to emulate traditional ResNet architectures.  Each GCN layer takes as input one input feature and outputs one input feature.  The final scalar output is computed by summing each column of $\mat{R}$ through a two layer feedforward neural network, with the first layer taking as input 9 features and outputting 5 features, and the second layer taking as input 5 features and outputting 1 feature.  This local transformation of each nodal value following by a global averaging removes any dependency for a fixed input size -- instead any sized graph and splitting could (in theory) be used.

\subsection{Convection-Diffusion Message Passing Network (MPNN)}\label{subsec:conv_mpnn}

The GCN layer, while simple to implement does not effectively learn edge data in a given neighborhood and thus cannot be considered to truly generalize the concept of ``convolution'' on a graph.  The Edge-Conditioned Convolution (ECC) layer, as defined by Simonovsky and Komodakis\cite{ecc}, attempts to learn optimal edge weights given an arbitrary neighborhood.  This approach is also referred to as a ``message-passing network'', as each connected node attempts to learn a ``message'' that is passed to the original.  The convolution operation is defined as:

\begin{equation}
  \left(\mat{H}^{(i+1)}\right)_j = \frac{1}{\abs{\mathcal{N}\left(j\right)}} \sum_{k\in\mathcal{N}\left(j\right)} F^{(i)}\left(e_{j,k}\right)\left(\mat{H}^{(i)}\right)_k + \vec{b}^{(i)}
\end{equation}
Where $\mat{H}^{\left(i\right)} \in \mathbb{R}^{n \times f^i}$ is the $i$'th hidden layer, $\mathcal{N}\left(i\right)$ is a map returning the neighborhood of vertex $i$ (including itself), $F \: : \: E \mapsto \mathbb{R}^{f^i f^{i-1} \times 1}$ is some function (in our case a feed-forward neural network) that outputs a learned weighting given an edge, and $\vec{b}^{(i)}$ is a bias term for layer $i$.  The notation $\left(\mat{H}^{\left(i\right)}\right)_j$ here denotes the $j$'th row of the $i$th hidden layer.

% \section{Main Contribution}\label{sec:main}

\section{Numerical Results}\label{sec:num}

\section{Conclusions}\label{sec:conc}

\bibliographystyle{siamplain}
\bibliography{references}

\end{document}
